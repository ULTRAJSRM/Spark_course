{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the [5] ins local mens that it will be a partition in our spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PartData') \\\n",
    "    .master('local[5]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a simple range and check the number of partitions.\n",
    "df = spark.range(0,20)\n",
    "df.rdd.getNumPartitions()\n",
    "# by default we get 5 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can specify the partition number directly using  parameters.\n",
    "# In this case we set 10 partition for rdd1.\n",
    "rdd1 = spark.sparkContext.parallelize((0,20),10)\n",
    "rdd1.getNumPartitions()\n",
    "# No we get 10 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddFromFile = spark.sparkContext \\\n",
    "    .textFile('/home/lastorder/Documents/curso-apache-spark-platzi/files/deporte.csv', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFromFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save our RDDs and Dataframes in an specific folder\n",
    "rddFromFile.saveAsTextFile('/home/lastorder/Documents/curso-apache-spark-platzi/output_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00002\tpart-00004  part-00006\tpart-00008  _SUCCESS\n",
      "part-00001  part-00003\tpart-00005  part-00007\tpart-00009\n"
     ]
    }
   ],
   "source": [
    "!ls /home/lastorder/Documents/curso-apache-spark-platzi/output_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte_id,deporte\n",
      "1,Basketball\n",
      "2,Judo\n",
      "3,Football\n",
      "4,Tug-Of-War\n",
      "5,Speed Skating\n",
      "6,Cross Country Skiing\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 /home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the values one option is to use:\n",
    "rdd = spark.sparkContext.wholeTextFiles('/home/lastorder/Documents/curso-apache-spark-platzi/output_files/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00000',\n",
       "  'deporte_id,deporte\\n1,Basketball\\n2,Judo\\n3,Football\\n4,Tug-Of-War\\n5,Speed Skating\\n6,Cross Country Skiing\\n'),\n",
       " ('file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00001',\n",
       "  '7,Athletics\\n8,Ice Hockey\\n9,Swimming\\n10,Badminton\\n11,Sailing\\n12,Biathlon\\n13,Gymnastics\\n14,Art Competitions\\n'),\n",
       " ('file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00002',\n",
       "  '15,Alpine Skiing\\n16,Handball\\n17,Weightlifting\\n18,Wrestling\\n19,Luge\\n20,Water Polo\\n'),\n",
       " ('file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00003',\n",
       "  '21,Hockey\\n22,Rowing\\n23,Bobsleigh\\n24,Fencing\\n25,Equestrianism\\n26,Shooting\\n27,Boxing\\n28,Taekwondo\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that this is no the best way to get partitional storage data.\n",
    "rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is en exeption where we can use collect in Spark\n",
    "rdd_list = rdd.mapValues(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we neet to extrect only the path to the files from the list\n",
    "rdd_list = [l[0] for l in rdd_list]\n",
    "rdd_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00000',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00001',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00002',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00003',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00004',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00005',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00006',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00007',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00008',\n",
       " 'file:/home/lastorder/Documents/curso-apache-spark-platzi/output_files/part-00009']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are indicate that join all files in the path with a comma\n",
    "rdd_fromList = spark.sparkContext.textFile(','.join(rdd_list), 10) \\\n",
    "    .map(lambda l: l.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deporte_id', 'deporte'],\n",
       " ['1', 'Basketball'],\n",
       " ['2', 'Judo'],\n",
       " ['3', 'Football']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the result\n",
    "rdd_fromList.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
