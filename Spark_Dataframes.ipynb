{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes\n",
    "**By Jorge S. Ruiz**\n",
    " - This is an introduction about how to use Dataframes in Spark.\n",
    " - Dataframes can be used as SQL tables.\n",
    " - Dataframes have better optimization because they use Catalyst as query optimization and Tugsten as execution engine.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Libraries for Datatypes (dataframes)\n",
    "from pyspark.sql.types import StructType, StructField \n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "# Library for SQL\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "spark = SparkContext(master='local', appName='Dataframes')\n",
    "# Initializing SQL Context\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte.csv\t deportistaError.csv  modelo_relacional.jpg\n",
      "deportista2.csv  evento.csv\t      paises.csv\n",
      "deportista.csv\t juegos.csv\t      resultados.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/lastorder/Documents/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",nombre_juego,annio,temporada,ciudad\n",
      "1,1896 Verano,1896,Verano,Athina\n",
      "2,1900 Verano,1900,Verano,Paris\n",
      "3,1904 Verano,1904,Verano,St. Louis\n"
     ]
    }
   ],
   "source": [
    "# Linux command to check the file content\n",
    "!head -n 4 /home/lastorder/Documents/curso-apache-spark-platzi/files/juegos.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the PATH to csv files.\n",
    "path = '/home/lastorder/Documents/curso-apache-spark-platzi/files/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a schema with the information of the columns:\n",
    "# Struct file helpus to indicate the parameters of the columns of the DF\n",
    "# The fields are, name of the column, datatype and if the column is an optional field.\n",
    "# False indicates that the column is a necessary field and true indicates that is optional.\n",
    "\n",
    "gameSchema = StructType([\n",
    "    StructField('game_id',IntegerType(),False),\n",
    "    StructField('year',StringType(),False),\n",
    "    StructField('season',StringType(),False),\n",
    "    StructField('city',StringType(),False)\n",
    "])\n",
    "\n",
    "# Now we can create the dataframe using our previous Schema.\n",
    "\n",
    "gameDF = sqlContext.read.schema(gameSchema).option('header','true') \\\n",
    "    .csv(path+'juegos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+--------+\n",
      "|game_id|         year|season|    city|\n",
      "+-------+-------------+------+--------+\n",
      "|      1|  1896 Verano|  1896|  Verano|\n",
      "|      2|  1900 Verano|  1900|  Verano|\n",
      "|      3|  1904 Verano|  1904|  Verano|\n",
      "|      4|  1906 Verano|  1906|  Verano|\n",
      "|      5|  1908 Verano|  1908|  Verano|\n",
      "|      6|  1912 Verano|  1912|  Verano|\n",
      "|      7|  1920 Verano|  1920|  Verano|\n",
      "|      8|1924 Invierno|  1924|Invierno|\n",
      "|      9|  1924 Verano|  1924|  Verano|\n",
      "|     10|1928 Invierno|  1928|Invierno|\n",
      "+-------+-------------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In dataframes we can use \"show\" to obtain a better data visualization\n",
    "gameDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.66:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Dataframes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access to Spark UI console, we use just \"spark\" command and click on Spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Extract from RDDs Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting 2 RDDs, the first one contains the header and the second one contains the data.\n",
    "OlimpicAthleteRDD = spark.textFile(path+'deportista.csv').map(lambda l : l.split(','))\n",
    "OlimpicAthleteRDD2 = spark.textFile(path+'deportista2.csv').map(lambda l : l.split(','))\n",
    "\n",
    "# To make a union between the RDDs we can use:\n",
    "OlimpicAthleteRDD = OlimpicAthleteRDD.union(OlimpicAthleteRDD2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135572"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make sure that the data is not corrupted, we can use count() to verify that spark is working correctly\n",
    "# with that data\n",
    "OlimpicAthleteRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the first 10 rows of the RDD, we use top function (similar to SQL)\n",
    "OlimpicAthleteRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the header of the RDD\n",
    "# 'iter' function, returns all values of we process in the function\n",
    "\n",
    "def removeHeader(index, iterator):\n",
    "    \"\"\"A fuction that removes the header from a dataset or RDD\"\"\"\n",
    "    return iter(list(iterator)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map the RDD assigning a index (this maps rows and columns per index)\n",
    "# Now we can use a function that will take action in all rows and columns on the RDD.\n",
    "OlimpicAthleteRDD_clean = OlimpicAthleteRDD.mapPartitionsWithIndex(removeHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278'],\n",
       " ['5', 'Christine Jacoba Aaftink', '2', '21', '185', '82', '705']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see if the header is gone\n",
    "OlimpicAthleteRDD_clean.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To transform the values of the RDD\n",
    "# For that we are goint to make a mapping\n",
    "\n",
    "OlimpicAthleteRDD_clean = OlimpicAthleteRDD_clean.map(lambda l: (\n",
    "    int(l[0]),\n",
    "    l[1],\n",
    "    int(l[2]),\n",
    "    int(l[3]),\n",
    "    int(l[4]),\n",
    "    float(l[5]),\n",
    "    int(l[6])    \n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ned to define a new Schema:\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('athlete_id', IntegerType(),False),\n",
    "    StructField('name', StringType(),False),\n",
    "    StructField('gender', IntegerType(),False),\n",
    "    StructField('age', IntegerType(),False),\n",
    "    StructField('height', IntegerType(),False),\n",
    "    StructField('weight', FloatType(),False),\n",
    "    StructField('team_id', IntegerType(),False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with sqlContext, using an existing RDD and a Schema\n",
    "AthleteDF = sqlContext.createDataFrame(OlimpicAthleteRDD_clean, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+---+------+------+-------+\n",
      "|athlete_id|                name|gender|age|height|weight|team_id|\n",
      "+----------+--------------------+------+---+------+------+-------+\n",
      "|         1|           A Dijiang|     1| 24|   180|  80.0|    199|\n",
      "|         2|            A Lamusi|     1| 23|   170|  60.0|    199|\n",
      "|         3| Gunnar Nielsen Aaby|     1| 24|     0|   0.0|    273|\n",
      "|         4|Edgar Lindenau Aabye|     1| 34|     0|   0.0|    278|\n",
      "|         5|Christine Jacoba ...|     2| 21|   185|  82.0|    705|\n",
      "|         6|     Per Knut Aaland|     1| 31|   188|  75.0|   1096|\n",
      "|         7|        John Aalberg|     1| 31|   183|  72.0|   1096|\n",
      "|         8|Cornelia Cor Aalt...|     2| 18|   168|   0.0|    705|\n",
      "|         9|    Antti Sami Aalto|     1| 26|   186|  96.0|    350|\n",
      "|        10|Einar Ferdinand E...|     1| 26|     0|   0.0|    350|\n",
      "+----------+--------------------+------+---+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To prove everything is correcto with our new DF, we can use show function\n",
    "AthleteDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
