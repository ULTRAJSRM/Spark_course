{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes\n",
    "**By Jorge S. Ruiz**\n",
    " - This is an introduction about how to use Dataframes in Spark.\n",
    " - Dataframes can be used as SQL tables.\n",
    " - Dataframes have better optimization because they use Catalyst as query optimization and Tugsten as execution engine.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Libraries for Datatypes (dataframes)\n",
    "from pyspark.sql.types import StructType, StructField \n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "# Library for SQL\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "spark = SparkContext(master='local', appName='Dataframes')\n",
    "# Initializing SQL Context\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte.csv\t deportistaError.csv  modelo_relacional.jpg\n",
      "deportista2.csv  evento.csv\t      paises.csv\n",
      "deportista.csv\t juegos.csv\t      resultados.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/lastorder/Documents/curso-apache-spark-platzi/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",nombre_juego,annio,temporada,ciudad\n",
      "1,1896 Verano,1896,Verano,Athina\n",
      "2,1900 Verano,1900,Verano,Paris\n",
      "3,1904 Verano,1904,Verano,St. Louis\n"
     ]
    }
   ],
   "source": [
    "# Linux command to check the file content\n",
    "!head -n 4 /home/lastorder/Documents/curso-apache-spark-platzi/files/juegos.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the PATH to csv files.\n",
    "path = '/home/lastorder/Documents/curso-apache-spark-platzi/files/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a schema with the information of the columns:\n",
    "# Struct file helpus to indicate the parameters of the columns of the DF\n",
    "# The fields are, name of the column, datatype and if the column is an optional field.\n",
    "# False indicates that the column is a necessary field and true indicates that is optional.\n",
    "\n",
    "gameSchema = StructType([\n",
    "    StructField('game_id',IntegerType(),False),\n",
    "    StructField('year',StringType(),False),\n",
    "    StructField('season',StringType(),False),\n",
    "    StructField('city',StringType(),False)\n",
    "])\n",
    "\n",
    "# Now we can create the dataframe using our previous Schema.\n",
    "\n",
    "gameDF = sqlContext.read.schema(gameSchema).option('header','true') \\\n",
    "    .csv(path+'juegos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+--------+\n",
      "|game_id|         year|season|    city|\n",
      "+-------+-------------+------+--------+\n",
      "|      1|  1896 Verano|  1896|  Verano|\n",
      "|      2|  1900 Verano|  1900|  Verano|\n",
      "|      3|  1904 Verano|  1904|  Verano|\n",
      "|      4|  1906 Verano|  1906|  Verano|\n",
      "|      5|  1908 Verano|  1908|  Verano|\n",
      "|      6|  1912 Verano|  1912|  Verano|\n",
      "|      7|  1920 Verano|  1920|  Verano|\n",
      "|      8|1924 Invierno|  1924|Invierno|\n",
      "|      9|  1924 Verano|  1924|  Verano|\n",
      "|     10|1928 Invierno|  1928|Invierno|\n",
      "+-------+-------------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In dataframes we can use \"show\" to obtain a better data visualization\n",
    "gameDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.66:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Dataframes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access to Spark UI console, we use just \"spark\" command and click on Spark UI\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Extract from RDDs Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting 2 RDDs, the first one contains the header and the second one contains the data.\n",
    "OlimpicAthleteRDD = spark.textFile(path+'deportista.csv').map(lambda l : l.split(','))\n",
    "OlimpicAthleteRDD2 = spark.textFile(path+'deportista2.csv').map(lambda l : l.split(','))\n",
    "\n",
    "# To make a union between the RDDs we can use:\n",
    "OlimpicAthleteRDD = OlimpicAthleteRDD.union(OlimpicAthleteRDD2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135572"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make sure that the data is not corrupted, we can use count() to verify that spark is working correctly\n",
    "# with that data\n",
    "OlimpicAthleteRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['99999', 'Alexander Grant Alick Rennie', '1', '32', '182', '71', '967'],\n",
       " ['99998', 'Robert John Bob Renney', '1', '21', '178', '90', '66'],\n",
       " ['99997', 'Thomas Renner', '1', '24', '183', '86', '71'],\n",
       " ['99996', 'Sara Renner', '2', '21', '168', '63', '174'],\n",
       " ['99995', 'Robert Renner', '1', '22', '182', '75', '944'],\n",
       " ['99994', 'Peter Campbell Renner', '1', '24', '186', '75', '716'],\n",
       " ['99993', 'Ingeborg Renner', '2', '22', '168', '60', '1150'],\n",
       " ['99992', 'Karlheinz Heinz Renneberg', '1', '25', '182', '87', '399'],\n",
       " ['99991', 'Paul Wisner Renne', '1', '24', '177', '73', '1096']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the first 10 rows of the RDD, we use top function (similar to SQL)\n",
    "OlimpicAthleteRDD.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the header of the RDD\n",
    "# 'iter' function, returns all values of we process in the function\n",
    "\n",
    "def removeHeader(index, iterator):\n",
    "    \"\"\"A fuction that removes the header from a dataset or RDD\"\"\"\n",
    "    return iter(list(iterator)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
