{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local', appName='TransformationsAndActions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "# In is important to check the datatype.\n",
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To vizualice and collect the data\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.66:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TransformationsAndActions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=TransformationsAndActions>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see that the first Job was successful we can use SparkUI\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deporte.csv\t deportistaError.csv  modelo_relacional.jpg\n",
      "deportista2.csv  evento.csv\t      paises.csv\n",
      "deportista.csv\t juegos.csv\t      resultados.csv\n"
     ]
    }
   ],
   "source": [
    "# reading the path where is the data\n",
    "# Linux commands inside Anaconda\n",
    "!ls /home/lastorder/Documents/curso-apache-spark-platzi/files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/lastorder/Documents/curso-apache-spark-platzi/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our first RDD\n",
    "OlimpicTeamsRDD = sc.textFile(path+\"paises.csv\")\\\n",
    "    .map(lambda line : line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG'],\n",
       " ['5', 'Afghanistan', 'AFG'],\n",
       " ['6', 'Akatonbo', 'IRL'],\n",
       " ['7', 'Alain IV', 'SUI'],\n",
       " ['8', 'Albania', 'ALB'],\n",
       " ['9', 'Alcaid', 'POR'],\n",
       " ['10', 'Alcyon-6', 'FRA'],\n",
       " ['11', 'Alcyon-7', 'FRA'],\n",
       " ['12', 'Aldebaran', 'ITA'],\n",
       " ['13', 'Aldebaran II', 'ITA'],\n",
       " ['14', 'Aletta', 'IRL']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take will show us an specific number of lines\n",
    "OlimpicTeamsRDD.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To count how many countries we have.\n",
    "# To select the country acronyms, we need to select the third column index:[0,1,2]\n",
    "# Similar to SQL, we use COUNT DISTINCT and select the column we need with a lambda\n",
    "OlimpicTeamsRDD.map(lambda x: (x[2])).distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', 1), ('AUT', 11), ('MEX', 9), ('ARG', 18), ('AFG', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark group all the data in a key-value format\n",
    "# the first value of the lambda will be the key value(x[2])(the value spark will use to group the data)\n",
    "# If we use len in mapvalues, Spark will show us the number of elements of the group list.\n",
    "OlimpicTeamsRDD.map(lambda x : (x[2],x[1])).groupByKey().mapValues(len).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sigla', ['equipo']),\n",
       " ('AUT',\n",
       "  ['30. Februar',\n",
       "   'Austria',\n",
       "   'Austria-1',\n",
       "   'Austria-2',\n",
       "   'Breslau',\n",
       "   'Brigantia',\n",
       "   'Donar III',\n",
       "   'Evita VI',\n",
       "   'May-Be 1960',\n",
       "   '\"R.-V. Germania; Leitmeritz\"',\n",
       "   'Surprise']),\n",
       " ('MEX',\n",
       "  ['A North American Team',\n",
       "   'Acipactli',\n",
       "   'Chamukina',\n",
       "   'Mexico',\n",
       "   'Mexico-1',\n",
       "   'Mexico-2',\n",
       "   'Nausikaa 4',\n",
       "   'Tlaloc',\n",
       "   'Xolotl']),\n",
       " ('ARG',\n",
       "  ['Acturus',\n",
       "   'Antares',\n",
       "   'Arcturus',\n",
       "   'Ardilla',\n",
       "   'Argentina',\n",
       "   'Argentina-1',\n",
       "   'Argentina-2',\n",
       "   'Blue Red',\n",
       "   'Covunco III',\n",
       "   'Cupidon III',\n",
       "   'Djinn',\n",
       "   'Gullvinge',\n",
       "   'Matrero II',\n",
       "   'Mizar',\n",
       "   'Pampero',\n",
       "   'Rampage',\n",
       "   'Tango',\n",
       "   'Wiking']),\n",
       " ('AFG', ['Afghanistan'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we use list in mapvalues, Spark will show us a list with all the values of the group\n",
    "OlimpicTeamsRDD.map(lambda x : (x[2],x[1])).groupByKey().mapValues(list).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4', 'Acturus', 'ARG'],\n",
       " ['37', 'Antares', 'ARG'],\n",
       " ['42', 'Arcturus', 'ARG'],\n",
       " ['43', 'Ardilla', 'ARG'],\n",
       " ['45', 'Argentina', 'ARG'],\n",
       " ['46', 'Argentina-1', 'ARG'],\n",
       " ['47', 'Argentina-2', 'ARG'],\n",
       " ['119', 'Blue Red', 'ARG'],\n",
       " ['238', 'Covunco III', 'ARG'],\n",
       " ['252', 'Cupidon III', 'ARG'],\n",
       " ['288', 'Djinn', 'ARG'],\n",
       " ['436', 'Gullvinge', 'ARG'],\n",
       " ['644', 'Matrero II', 'ARG'],\n",
       " ['672', 'Mizar', 'ARG'],\n",
       " ['774', 'Pampero', 'ARG'],\n",
       " ['843', 'Rampage', 'ARG'],\n",
       " ['1031', 'Tango', 'ARG'],\n",
       " ['1162', 'Wiking', 'ARG']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE!!: the Use of Collect is not a good practice because when you use it, collect all the data around the\n",
    "# all the clusters in the system (all computers that are running spark), so if you had a lot of RDDs with\n",
    "# TB of data, collect will capture all that data from all the CPUS and it will send it to the Spark Driver\n",
    "# (the central computer), that can couse several problems to the system.\n",
    "\n",
    "ArgTeams = OlimpicTeamsRDD.filter(lambda l : 'ARG' in l)\n",
    "ArgTeams.collect()\n",
    "\n",
    "# NEVER USE COLLECT UNTIL YOU KNOW THAT THE DATA IS LOCAL AND SMALL !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count might take several minutes if the Data is really big, in that cases we can use countApprox\n",
    "OlimpicTeamsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countApprox will take a maximum num of second before end the process, if the process end before the number\n",
    "# of second we give it then it will show the result in that moment.\n",
    "# If the time ends and the process is still runing, then it will show us the total count until that moment\n",
    "OlimpicTeamsRDD.countApprox(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
